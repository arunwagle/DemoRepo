{
    "metadata": {
        "language_info": {
            "name": "python", 
            "file_extension": ".py", 
            "codemirror_mode": {
                "name": "ipython", 
                "version": 2
            }, 
            "mimetype": "text/x-python", 
            "nbconvert_exporter": "python", 
            "pygments_lexer": "ipython2", 
            "version": "2.7.11"
        }, 
        "kernelspec": {
            "name": "python2-spark20", 
            "display_name": "Python 2 with Spark 2.0", 
            "language": "python"
        }
    }, 
    "cells": [
        {
            "execution_count": 1, 
            "source": "!pip install --user xlrd", 
            "metadata": {}, 
            "cell_type": "code", 
            "outputs": [
                {
                    "name": "stdout", 
                    "output_type": "stream", 
                    "text": "Requirement already satisfied: xlrd in /gpfs/global_fs01/sym_shared/YPProdSpark/user/s1df-1767d8774d3251-73caa6cfaa60/.local/lib/python2.7/site-packages\r\n"
                }
            ]
        }, 
        {
            "execution_count": 2, 
            "source": "# Setup constants if any\n# FUNDS ID\nFUNDS_ID_LIST = ['I-CJF','I-MG1','I-SQGFS']\n\n", 
            "metadata": {
                "collapsed": true
            }, 
            "cell_type": "code", 
            "outputs": []
        }, 
        {
            "execution_count": 3, 
            "source": "import pandas as pd\nfrom io import BytesIO\nimport requests\nimport json\nimport xlrd \n\nfrom pyspark.sql.functions import *\nfrom pyspark.sql.types import *\n\nfrom datetime import datetime\nfrom dateutil.parser import parse\n\nfrom ingest.Connectors import Connectors", 
            "metadata": {
                "collapsed": true
            }, 
            "cell_type": "code", 
            "outputs": []
        }, 
        {
            "execution_count": 4, 
            "source": "# The code was removed by DSX for sharing.", 
            "metadata": {
                "collapsed": true
            }, 
            "cell_type": "code", 
            "outputs": []
        }, 
        {
            "execution_count": 5, 
            "source": "# The code was removed by DSX for sharing.", 
            "metadata": {
                "collapsed": true
            }, 
            "cell_type": "code", 
            "outputs": []
        }, 
        {
            "execution_count": 6, 
            "source": "fundNavDF = pd.read_excel(getFileFromObjectStorage('MizuhoPOC', 'Fund_NAV_MAI_DailyPLfiles_20170731.xlsm'), \n                              header=[0], skipinitialspace=True, \n                              skiprows=8, \n                              usecols=['Portfolio', 'Base CCY','NAV (Base CCY)']).dropna(axis=[0,1])\n\nfundNavRenamedDF = fundNavDF.rename(index=str, columns={\"Portfolio\":\"FUND_ID\",\"Base CCY\": \"BASE_CURR\",\"NAV (Base CCY)\": \"NAV_BASE_CCY\"})\n\nfundNavRenamedDF[['NAV_BASE_CCY']] = fundNavRenamedDF[['NAV_BASE_CCY']].astype(float)\n\nasOfDate = pd.to_datetime('today').strftime(\"%Y-%m-%d\")\nprint asOfDate\n\nfundNavRenamedDF.dtypes\n\n", 
            "metadata": {
                "scrolled": true
            }, 
            "cell_type": "code", 
            "outputs": [
                {
                    "name": "stdout", 
                    "output_type": "stream", 
                    "text": "2017-10-09\n"
                }, 
                {
                    "execution_count": 6, 
                    "metadata": {}, 
                    "data": {
                        "text/plain": "FUND_ID          object\nBASE_CURR        object\nNAV_BASE_CCY    float64\ndtype: object"
                    }, 
                    "output_type": "execute_result"
                }
            ]
        }, 
        {
            "execution_count": 7, 
            "source": "spark = SparkSession.builder.getOrCreate()  \n\ndef build_schema():\n    \"\"\"Build and return a schema to use for the sample data.\"\"\"\n    schema = StructType(\n        [            \n            StructField(\"FUND_ID\",  StringType(), False),\n            StructField(\"BASE_CURR\", StringType(), True),\n            StructField(\"NAV_BASE_CCY\", DoubleType(), True)\n        ]\n    )\n    return schema\n\n\nfundNavDFSparkDF = spark.createDataFrame(fundNavRenamedDF, schema=build_schema()) \\\n                                .withColumn(\"AS_OF_DATE\", lit(asOfDate).cast(\"date\"))\\\n                                .select(col(\"FUND_ID\"),col(\"AS_OF_DATE\"), col(\"BASE_CURR\"), col(\"NAV_BASE_CCY\").cast(DecimalType(20,10)))\n\n\nfundNavDFSparkDF.printSchema()\nfundNavDFSparkDF.head(1)\n", 
            "metadata": {}, 
            "cell_type": "code", 
            "outputs": [
                {
                    "name": "stdout", 
                    "output_type": "stream", 
                    "text": "root\n |-- FUND_ID: string (nullable = false)\n |-- AS_OF_DATE: date (nullable = true)\n |-- BASE_CURR: string (nullable = true)\n |-- NAV_BASE_CCY: decimal(20,10) (nullable = true)\n\n"
                }, 
                {
                    "execution_count": 7, 
                    "metadata": {}, 
                    "data": {
                        "text/plain": "[Row(FUND_ID=u'Mizuho Alternative Investments, LLC', AS_OF_DATE=datetime.date(2017, 10, 9), BASE_CURR=u'USD', NAV_BASE_CCY=Decimal('989399546.1000000000'))]"
                    }, 
                    "output_type": "execute_result"
                }
            ]
        }, 
        {
            "execution_count": 9, 
            "source": "dashDBloadOptions = { \n                    Connectors.DASHDB.HOST              : dashCredentials[\"host\"],\n                    Connectors.DASHDB.DATABASE          : dashCredentials[\"db\"],\n                    Connectors.DASHDB.USERNAME          : dashCredentials[\"username\"],\n                    Connectors.DASHDB.PASSWORD          : dashCredentials[\"password\"],\n                    Connectors.DASHDB.SOURCE_TABLE_NAME : dashCredentials[\"REF_FUND_TABLE\"],\n}\n\nrefFundDF = sqlContext.read.format(\"com.ibm.spark.discover\").options(**dashDBloadOptions).load()\nrefFundDF.printSchema()\nrefFundDF.show(1)", 
            "metadata": {}, 
            "cell_type": "code", 
            "outputs": [
                {
                    "name": "stdout", 
                    "output_type": "stream", 
                    "text": "root\n |-- ID: string (nullable = false)\n\n+-----+\n|   ID|\n+-----+\n|I-CJF|\n+-----+\nonly showing top 1 row\n\n"
                }
            ]
        }, 
        {
            "execution_count": 10, 
            "source": "fundNavDFJoinSparkDF = fundNavDFSparkDF.join(refFundDF, \n                                               fundNavDFSparkDF.FUND_ID == refFundDF.ID, \"inner\")\\\n                                        .select(\n                                            refFundDF.ID.alias(\"FUND_ID\"),\n                                            fundNavDFSparkDF.BASE_CURR,\n                                            fundNavDFSparkDF.NAV_BASE_CCY,\n                                            fundNavDFSparkDF.AS_OF_DATE,                                                                                              \n                                            ).orderBy(\"FUND_ID\")\n\nfundNavDFJoinSparkDF.show(19)", 
            "metadata": {}, 
            "cell_type": "code", 
            "outputs": [
                {
                    "name": "stdout", 
                    "output_type": "stream", 
                    "text": "+---------+---------+--------------------+----------+\n|  FUND_ID|BASE_CURR|        NAV_BASE_CCY|AS_OF_DATE|\n+---------+---------+--------------------+----------+\n|   I-AQUA|      USD| 29577088.9600000000|2017-10-09|\n|    I-CGF|      USD|  -388666.1400000000|2017-10-09|\n|    I-CJF|      USD| 56683885.3700000000|2017-10-09|\n|    I-MG1|      USD|106622170.6200000000|2017-10-09|\n|  I-RLLAF|      USD| 15010831.0900000000|2017-10-09|\n|  I-SQGFS|      USD| 50374707.5700000000|2017-10-09|\n| I-SQGFSO|      USD|  6457979.2300000000|2017-10-09|\n|I-WEUBEAR|      JPY|936984347.3400000000|2017-10-09|\n|I-WEUBULL|      JPY|                null|2017-10-09|\n|I-WGEBEAR|      JPY|806685713.8000000000|2017-10-09|\n|I-WGEBULL|      JPY|1081921808.370000...|2017-10-09|\n|   I-WGEF|      USD| 10003797.5000000000|2017-10-09|\n|I-WMOBEAR|      JPY|1007773488.400000...|2017-10-09|\n|I-WMOBULL|      JPY|1029205326.220000...|2017-10-09|\n|I-WUEBEAR|      JPY|1002523582.030000...|2017-10-09|\n|I-WUEBULL|      JPY|4894914735.100000...|2017-10-09|\n|I-WUSBEAR|      JPY|1203354827.230000...|2017-10-09|\n|I-WUSBULL|      JPY|990963508.5200000000|2017-10-09|\n| I-WUSIGX|      JPY|                null|2017-10-09|\n+---------+---------+--------------------+----------+\n\n"
                }
            ]
        }, 
        {
            "execution_count": 162, 
            "source": "\n\n# Connection to Dash DB for writing the data\ndashdbsaveoption = {\n                     Connectors.DASHDB.HOST              : dashCredentials[\"host\"],\n                     Connectors.DASHDB.DATABASE          : dashCredentials[\"db\"],\n                     Connectors.DASHDB.USERNAME          : dashCredentials[\"username\"],\n                     Connectors.DASHDB.PASSWORD          : dashCredentials[\"password\"],\n                     Connectors.DASHDB.TARGET_TABLE_NAME : dashCredentials[\"tableName\"],\n                     Connectors.DASHDB.TARGET_WRITE_MODE : 'merge' \n}\n\nfundNavDashDBDF = fundNavDFJoinSparkDF.write.format(\"com.ibm.spark.discover\").options(**dashdbsaveoption).save()\n", 
            "metadata": {
                "collapsed": true
            }, 
            "cell_type": "code", 
            "outputs": []
        }, 
        {
            "execution_count": null, 
            "source": "", 
            "metadata": {
                "collapsed": true
            }, 
            "cell_type": "code", 
            "outputs": []
        }
    ], 
    "nbformat": 4, 
    "nbformat_minor": 1
}